% Abstract

\noindent
In deep-learning, representation-learning autoencoder networks demonstrate promising potential for fast and accurate transformation of data between highly compressed and uncompressed formats.
In this paper, I evaluate the capacity of various autoencoder network architectures to compress and subsequently reconstruct high-dimensional images of handwritten digits.
Using a parsimonious empirical framework, I evaluate a range of autoencoder architectures with distinct combinations of compression-ratio and bias-unit parameterisations, and document that reconstruction accuracy of encoded images monotonically diminishes with the imposition of higher compression-ratios.
Moreover, I demonstrate that the reconstruction accuracy of input images depends on the complexity of structures in the original image.
Results from this paper can be referenced to contextualise hyperparameter selection when parameterising of autoencoder networks.
% Introduction


\noindent
Ours is the age of information: where digital data is ubiquitous, and computational resources are in consequently constrained supply.
Now, more than ever before, individuals possess unprecedented access to vast quantities of data. 
However, as the volume of information available for consumption increases, so too does the demand for more efficient and cost-effective mechanisms for information exchange. 
Regardless of the quantity or utility of information, no one will ask for it, learn from it, or act upon it unless it can be acquired in a timely and inexpensive manner. 
Indeed, a fundamental problem persists in the efficient management of data to limit the computational expense of information storage and exchange.
% Indeed, the speed at which information can be exchanged is constrained by the limits of our computational systems, and the ingenuity of data-management techniques to reduce the computational expense of information exchange.

On this account, data-compression techniques seek to alleviate the computational burden of sending and storing digital information by compressing data to representations of reduced size.
Most practical applications of compression occur through lossless techniques (such as ZIP and gzip) which enable perfect reconstruction of input data from compressed formats.
While lossless compression offers uncompromised data-accuracy between compressed and uncompressed formats, recent reports show that practical applications rarely achieve compression ratios in excess of two- to four-times the size of the original data \cite{mittal2015survey}.
In contrast, lossy approaches to data compression are able to achieve exceptionally high compression ratios through the use of inexact approximations and partial data exclusion techniques.
While compression achieved in this way invariably degrades data-accuracy, consequent reductions in file size reduce the computational expense of storing and sending information, and may justify the trade-off.
Moreover, well-designed approaches to lossy compression can achieve remarkably high compression-ratios before degradation is noticed by end-users.
Accordingly, in scenarios where speed of information exchange outweighs minor degradation in data-accuracy (such as in the case of real-time multimedia communication), lossy compression becomes an attractive approach. 

Among the most successful non-probabalistic approaches to lossy compression occur through the use of autoencoder networks.
Autoencoders are representation-learning applications of unsupervised artificial neural networks (ANNs), that seek to learn mappings of high-dimensional data to meaningful lower-dimensional representations.
By learning fundamental representations of data, autoencoders enable fast transformation of data between highly compressed and uncompressed formats.
This, in turn, enables computationally inexpensive storage and exchange of large files.
Indeed, several works demonstrate the ability of autoencoders to learn compressed representations of image data, with promising reconstruction accuracy \cite{cheng2018deep, toderici2017full, balle2016end}.
Despite the interest autoencoder networks have generated, little empirical evidence has emerged to this date on the effect of variation in model hyperparameters.
Recognising this, the analysis in this note seeks to evaluate the capacity of various autoencoder network architectures to compress and subsequently reconstruct high-dimensional images of handwritten digits. 
To this end, the present analysis will investigate variations in reconstruction-error resulting from: (1) modifications to compression-ratios enforced by the autoencoder; and (2) the inclusion of bias nodes within the network. In particular, the analysis will address the following research questions:

\begin{itemize}
	\item[] \textbf{RQ1}: How does the selection of compression ratio influence reconstruction accuracy of compressed images?
	\item[] \textbf{RQ2}: How does the inclusion/exclusion of bias nodes affect reconstruction accuracy of compressed images?
\end{itemize}

\noindent
The remainder of this paper is structured as follows. 
Section 2 briefly outlines the intuition behind autoencoder networks.
Section 3 describes the dataset used in this analysis, and provides an overview of the general experimental framework.
Section 4 presents and discusses the results. 
Finally, Section 5 concludes the paper.



